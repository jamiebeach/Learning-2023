{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Algebra in Machine Learning\n",
    "\n",
    "#### Introduction to Matrix Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Terminology\n",
    "To understand matrix algebra, it's important to be familiar with its basic terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Matrix\n",
    "\n",
    "A matrix is a collection of numbers arranged into a fixed number of rows and columns. It is typically denoted by a capital letter (e.g., A, B, C).\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "A matrix A can be represented as:\n",
    "\n",
    "$A = \\begin{pmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector\n",
    "\n",
    "A vector is a special type of matrix with only one row or one column. Vectors are often used to represent data points or coefficients in machine learning.\n",
    "\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "A column vector v:\n",
    "$v = \\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n} \\end{pmatrix}$\n",
    "\n",
    "A row vector u:\n",
    "$u = \\begin{pmatrix} u_{1} & u_{2} & \\cdots & u_{n} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column vector:\n",
      " [[1]\n",
      " [2]\n",
      " [3]]\n",
      "Row vector:\n",
      " [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([[1], [2], [3]])  # Column vector\n",
    "u = np.array([1, 2, 3])       # Row vector\n",
    "print(\"Column vector:\\n\", v)\n",
    "print(\"Row vector:\\n\", u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Element\n",
    "\n",
    "An element or entry of a matrix is an individual number within the matrix. It is usually denoted by a lowercase letter with two subscript indices (e.g., a_ij represents the element in the i-th row and j-th column of matrix A).\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "The element in the i-th row and j-th column of matrix A is denoted as $a_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Accessing the element at row index 1 and column index 2 of matrix A\n",
    "element = A[1, 2]\n",
    "print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Row and Column\n",
    "\n",
    "A row is a horizontal line of elements in a matrix, while a column is a vertical line of elements. The size of a matrix is often defined by its number of rows and columns.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "The i-th row and j-th column of matrix A can be represented as row vectors:\n",
    "\n",
    "$\\text{Row } i: A_{i, :} = \\begin{pmatrix} a_{i1} & a_{i2} & \\cdots & a_{in} \\end{pmatrix}$\n",
    "\n",
    "$\\text{Column } j: A_{:, j} = \\begin{pmatrix} a_{1j} \\\\ a_{2j} \\\\ \\vdots \\\\ a_{mj} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2: [4 5 6]\n",
      "Column 3: [3 6 9]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the second row and third column of matrix A\n",
    "row = A[1, :]\n",
    "column = A[:, 2]\n",
    "print(\"Row 2:\", row)\n",
    "print(\"Column 3:\", column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dimension\n",
    "\n",
    "The dimension or size of a matrix is given by the number of rows and columns it contains, typically denoted as m × n, where m is the number of rows, and n is the number of columns.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "The dimension of matrix A is m × n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of A: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "dimensions = A.shape\n",
    "print(\"Dimensions of A:\", dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Transpose\n",
    "\n",
    "The transpose of a matrix is a new matrix obtained by flipping it over its diagonal. Essentially, the row and column indices of each element are swapped. The transpose of matrix A is denoted as A^T.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "The transpose of matrix A, denoted as $A^T$, is:\n",
    "\n",
    "$A^T = \\begin{pmatrix} a_{11} & a_{21} & \\cdots & a_{m1} \\\\ a_{12} & a_{22} & \\cdots & a_{m2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{1n} & a_{2n} & \\cdots & a_{mn} \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose of A:\n",
      " [[1 4 7]\n",
      " [2 5 8]\n",
      " [3 6 9]]\n"
     ]
    }
   ],
   "source": [
    "A_transpose = A.T\n",
    "print(\"Transpose of A:\\n\", A_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Square Matrix, Identity Matrix, and Inverse Matrix\n",
    "\n",
    "- A square matrix is a matrix with the same number of rows and columns. Special operations and properties apply to square matrices in matrix algebra.\n",
    "- An identity matrix is a square matrix with ones on the diagonal and zeros elsewhere. It is denoted as I and acts as the multiplicative identity in matrix operations.\n",
    "- The inverse of a matrix A is another matrix, denoted as A^-1, such that when it is multiplied with A, it results in the identity matrix. Not all matrices have inverses.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "\n",
    "- Square Matrix: A matrix with size n × n.\n",
    "- Identity Matrix: $I_n = \\begin{pmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{pmatrix}$\n",
    "- Inverse Matrix: If A is a square matrix, then its inverse $A^{-1}$ satisfies $AA^{-1} = A^{-1}A = I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Square Matrix B:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "Identity Matrix:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "Inverse of B:\n",
      " [[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Square Matrix\n",
    "B = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Identity Matrix\n",
    "I = np.eye(2)\n",
    "\n",
    "# Inverse Matrix\n",
    "B_inv = np.linalg.inv(B)\n",
    "\n",
    "print(\"Square Matrix B:\\n\", B)\n",
    "print(\"Identity Matrix:\\n\", I)\n",
    "print(\"Inverse of B:\\n\", B_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices and Vectors\n",
    "\n",
    "#### Matrix Operations\n",
    "\n",
    "##### 1. Matrix Addition and Subtraction\n",
    "\n",
    "**Mathematical Notation & Example**: \n",
    "\n",
    "Given two matrices $A$ and $B$ of the same dimension $m \\times n$:\n",
    "\n",
    "$A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}$\n",
    "\n",
    "Their sum $C = A + B$ is:\n",
    "\n",
    "$C = \\begin{pmatrix} 1+5 & 2+6 \\\\ 3+7 & 4+8 \\end{pmatrix} = \\begin{pmatrix} 6 & 8 \\\\ 10 & 12 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition:\n",
      " [[ 6  8]\n",
      " [10 12]]\n",
      "Matrix Subtraction:\n",
      " [[-4 -4]\n",
      " [-4 -4]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix Addition\n",
    "C = A + B\n",
    "print(\"Matrix Addition:\\n\", C)\n",
    "\n",
    "# Matrix Subtraction\n",
    "D = A - B\n",
    "print(\"Matrix Subtraction:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 2. Scalar Multiplication\n",
    "\n",
    "**Mathematical Notation & Example**: \n",
    "\n",
    "Given a matrix $A$ and a scalar $k$:\n",
    "\n",
    "$A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad k = 3$\n",
    "\n",
    "The product $C = kA$ is:\n",
    "\n",
    "$C = 3 \\cdot \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} = \\begin{pmatrix} 3 & 6 \\\\ 9 & 12 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar Multiplication:\n",
      " [[ 3  6]\n",
      " [ 9 12]]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "\n",
    "# Scalar Multiplication\n",
    "C = k * A\n",
    "print(\"Scalar Multiplication:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Matrix Multiplication\n",
    "\n",
    "**Mathematical Notation & Example**: \n",
    "\n",
    "Given two matrices $A$ (size $m \\times n$) and $B$ (size $n \\times p$):\n",
    "\n",
    "$A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}$\n",
    "\n",
    "Their product $AB$ is:\n",
    "\n",
    "$AB = \\begin{pmatrix} 1\\cdot5 + 2\\cdot7 & 1\\cdot6 + 2\\cdot8 \\\\ 3\\cdot5 + 4\\cdot7 & 3\\cdot6 + 4\\cdot8 \\end{pmatrix} = \\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication:\n",
      " [[19 22]\n",
      " [43 50]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix Multiplication\n",
    "C = np.dot(A, B)\n",
    "print(\"Matrix Multiplication:\\n\", C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Spaces\n",
    "\n",
    "##### 1. Vector Spaces\n",
    "\n",
    "**Mathematical Notation & Example**: \n",
    "\n",
    "A vector space over $\\mathbb{R}$ includes vectors that can be scaled and added. For example, vectors $v_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$ in $\\mathbb{R}^2$ can be added and scaled to form new vectors in the same space.\n",
    "\n",
    "In Python, we typically work with vector spaces using NumPy arrays. For instance, the set of all 2-dimensional real vectors forms a vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Addition: [4 6]\n",
      "Scalar Multiplication: [2 4]\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1, 2])\n",
    "v2 = np.array([3, 4])\n",
    "\n",
    "# Vector Addition\n",
    "v3 = v1 + v2\n",
    "\n",
    "# Scalar Multiplication\n",
    "v4 = 2 * v1\n",
    "\n",
    "print(\"Vector Addition:\", v3)\n",
    "print(\"Scalar Multiplication:\", v4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Basis and Dimensionality\n",
    "\n",
    "The basis of a vector space is a set of linearly independent vectors that span the entire space. The number of vectors in the basis is the dimension of the vector space.\n",
    "\n",
    "**Mathematical Notation & Example**: \n",
    "\n",
    "In $\\mathbb{R}^2$, the standard basis is $\\{e_1, e_2\\}$ where $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Any vector $v = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$ in $\\mathbb{R}^2$ can be expressed as a linear combination of $e_1$ and $e_2$: $v = a \\cdot e_1 + b \\cdot e_2$.\n",
    "\n",
    "Demonstrating basis and dimensionality explicitly in Python is more abstract, as it involves concepts like linear independence and span. However, we can think of the standard basis in R^2 (real 2D space) as two vectors, [1, 0] and [0, 1]. Any 2D vector can be represented as a linear combination of these two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination of basis vectors: [3 4]\n"
     ]
    }
   ],
   "source": [
    "# Standard basis in R^2\n",
    "e1 = np.array([1, 0])\n",
    "e2 = np.array([0, 1])\n",
    "\n",
    "# Any vector in R^2 can be represented as a combination of e1 and e2\n",
    "v = np.array([3, 4])\n",
    "alpha, beta = v\n",
    "combination = alpha * e1 + beta * e2\n",
    "print(\"Combination of basis vectors:\", combination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Dot Product\n",
    "\n",
    "The dot product is a crucial operation in vector algebra, often used in machine learning for calculating angles and projections between vectors.\n",
    "\n",
    "**Mathematical Notation & Example**: \n",
    "\n",
    "The dot product of two vectors $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$ and $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ in $\\mathbb{R}^2$ is calculated as:\n",
    "\n",
    "$u \\cdot v = u_1v_1 + u_2v_2$\n",
    "\n",
    "For example, if $u = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$ and $v = \\begin{pmatrix} 4 \\\\ 2 \\end{pmatrix}$, then:\n",
    "\n",
    "$u \\cdot v = 1\\cdot4 + 3\\cdot2 = 4 + 6 = 10$\n",
    "\n",
    "The dot product extends to higher dimensions similarly. It is a fundamental operation in many machine learning algorithms, particularly those involving geometric interpretations of data, such as in the case of support vector machines or neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot Product: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define two vectors\n",
    "u = np.array([1, 3])\n",
    "v = np.array([4, 2])\n",
    "\n",
    "# Compute the dot product\n",
    "dot_product = np.dot(u, v)\n",
    "\n",
    "print(\"Dot Product:\", dot_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Types of Matrices\n",
    "\n",
    "#### Identity and Diagonal Matrices\n",
    "\n",
    "##### Identity Matrices\n",
    "**Mathematical Notation**: \n",
    "An identity matrix $I_n$ of size $n \\times n$ is a square matrix with ones on the diagonal and zeros elsewhere.\n",
    "$$I_n = \\begin{pmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\end{pmatrix}$$\n",
    "\n",
    "**Properties**: \n",
    "- Multiplying any matrix $A$ by the identity matrix $I$ (of appropriate size) results in $A$ itself: $AI = IA = A$.\n",
    "- It serves as the multiplicative identity in matrix operations.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "I = np.eye(3)  # Create a 3x3 identity matrix\n",
    "print(\"Identity Matrix:\\n\", I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diagonal Matrices\n",
    "**Mathematical Notation**: \n",
    "A diagonal matrix $D$ is a matrix in which the entries outside the main diagonal are all zero.\n",
    "\n",
    "$$D = \\begin{pmatrix} d_1 & 0 & \\cdots & 0 \\\\ 0 & d_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & d_n \\end{pmatrix}$$\n",
    "\n",
    "**Properties**: \n",
    "- Diagonal matrices are a generalization of the identity matrix.\n",
    "- They are easy to invert (if non-singular); the inverse of $D$ is simply a diagonal matrix with the reciprocals of the original diagonal elements.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.diag([1, 2, 3])  # Create a diagonal matrix\n",
    "print(\"Diagonal Matrix:\\n\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric and Skew-Symmetric Matrices\n",
    "\n",
    "##### Symmetric Matrices\n",
    "**Mathematical Notation**: \n",
    "A matrix $A$ is symmetric if $A = A^T$.\n",
    "$\\text{If } A = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}, \\text{ then } A^T = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$\n",
    "\n",
    "**Properties**: \n",
    "- Symmetric matrices are equal to their transposes.\n",
    "- They often represent self-adjoint operators over a real inner product space.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 3], [3, 4]])\n",
    "print(\"Symmetric Matrix:\\n\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Skew-Symmetric Matrices\n",
    "**Mathematical Notation**: \n",
    "A matrix $A$ is skew-symmetric if $A^T = -A$.\n",
    "$\\text{If } A = \\begin{pmatrix} 0 & a \\\\ -a & 0 \\end{pmatrix}, \\text{ then } A^T = \\begin{pmatrix} 0 & -a \\\\ a & 0 \\end{pmatrix}$\n",
    "\n",
    "**Properties**: \n",
    "- The diagonal elements of a skew-symmetric matrix are always zero.\n",
    "- Useful in various applications, including the study of motion and angular velocity.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_skew = np.array([[0, 1], [-1, 0]])\n",
    "print(\"Skew-Symmetric Matrix:\\n\", A_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orthogonal and Orthonormal Matrices\n",
    "\n",
    "##### Orthogonal Matrices\n",
    "**Mathematical Notation**: \n",
    "A matrix $Q$ is orthogonal if its transpose is equal to its inverse: \n",
    "\n",
    "$$Q^TQ = QQ^T = I$$\n",
    "\n",
    "**Properties**: \n",
    "- The rows and columns of an orthogonal matrix are orthonormal vectors.\n",
    "- They preserve the dot product, hence lengths and angles, making them key in rotations and reflections.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[0, -1], [1, 0]])\n",
    "print(\"Orthogonal Matrix:\\n\", Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Orthonormal Matrices\n",
    "**Mathematical Notation**: \n",
    "A matrix is orthonormal if its columns (and rows) are unit vectors and orthogonal to each other.\n",
    "\n",
    "**Properties**: \n",
    "- An orthonormal matrix is always an orthogonal matrix, but the converse is not necessarily true.\n",
    "- They are used to simplify computations in linear algebra, particularly in vector transformations.\n",
    "\n",
    "**Python Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an orthonormal matrix using the Gram-Schmidt process is complex.\n",
    "# Here, we use a simple 2x2 example.\n",
    "Q_orthonormal = np.array([[1/np.sqrt(2), -1/np.sqrt(2)], [1/np.sqrt(2), 1/np.sqrt(2)]])\n",
    "print(\"Orthonormal Matrix:\\n\", Q_orthonormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Decomposition\n",
    "\n",
    "#### Eigenvalues and Eigenvectors\n",
    "\n",
    "##### ELI5 Explanation\n",
    "\n",
    "Imagine you have a magical shape-shifting machine. You can put any object into this machine, like a rubber ball, and the machine will stretch or shrink it, maybe even change its direction. However, there are some special objects that, when you put them in the machine, only get bigger or smaller but don't change their direction. These special objects are like eigenvectors, and the amount by which they stretch or shrink is the eigenvalue.\n",
    "\n",
    "Let's break it down:\n",
    "\n",
    "1. **Eigenvectors**: These are like the special objects that don't change their direction when put in the shape-shifting machine. In mathematical terms, when you apply a matrix (which represents the machine) to an eigenvector, the vector may get stretched or compressed, but it doesn't rotate or change its direction.\n",
    "\n",
    "2. **Eigenvalues**: This is the amount by which the eigenvector gets stretched or shrunk when you put it through the matrix. If the eigenvalue is large, the eigenvector stretches a lot. If it's small, the eigenvector shrinks. And if the eigenvalue is negative, the eigenvector flips direction as well as stretches or shrinks.\n",
    "\n",
    "In more practical terms, in the world of data and machine learning, eigenvalues and eigenvectors are used to understand the properties of different transformations and systems. For example, they can help identify which directions in a dataset are the most important (like which way a cloud of data points stretches out the most). This is super useful in things like Principal Component Analysis (PCA), where you want to find the best way to look at complex data to make sense of it.\n",
    "\n",
    "So, in summary, eigenvalues and eigenvectors are about finding the special directions in which a transformation acts simply by stretching or shrinking, without twisting or rotating. This concept helps in simplifying and understanding complex data transformations.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "For a square matrix $A$, an eigenvector $v$ and its corresponding eigenvalue $\\lambda$ satisfy the equation $Av = \\lambda v$.\n",
    "\n",
    "**Significance in Machine Learning**: \n",
    "- Eigenvalues and eigenvectors are crucial in understanding the properties of a matrix, often used in algorithms like Principal Component Analysis (PCA) for dimensionality reduction.\n",
    "- They help in identifying the directions in which a transformation represented by the matrix stretches or compresses.\n",
    "\n",
    "**Python Example**:\n",
    "```python\n",
    "A = np.array([[4, 2], [1, 3]])\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)\n",
    "```\n",
    "\n",
    "#### Singular Value Decomposition (SVD)\n",
    "\n",
    "##### ELI5 Explanation :\n",
    "\n",
    "Imagine you have a bunch of photographs of various objects, and you want to organize them in a way that highlights their similarities and differences. SVD is like a smart photo album that can do this for you.\n",
    "\n",
    "First, SVD takes each photo and breaks it down into three key components:\n",
    "\n",
    "1. **Shadows (U matrix)**: This part captures the shapes or outlines of the objects in your photos. It's like looking at the shadows cast by the objects under a lamp. These shadows tell you about the structure or form of the objects.\n",
    "\n",
    "2. **Brightness or Importance (Σ matrix)**: This part is a list that tells you how important or prominent each shadow is. Think of it as a way to rank the shadows by their strength or clarity. The stronger or clearer shadows are more important in understanding the overall picture.\n",
    "\n",
    "3. **Colors and Textures (V matrix)**: This part captures the colors and textures in your photos. It's like looking at the different color patterns and textures without worrying about the shape of the objects.\n",
    "\n",
    "Now, why is this useful? Because it helps you understand and organize your photos (or data) efficiently:\n",
    "\n",
    "- Maybe you want to find the most common shapes (shadows) across all your photos. SVD helps you see these common patterns.\n",
    "- Or, you might be interested in reducing the space your photo album takes up. SVD can help you keep the most important parts (the strongest shadows and most distinct colors) and get rid of less important details, making your album more compact without losing its essence.\n",
    "\n",
    "In machine learning and data science, SVD does something similar with data. It breaks down complex data sets into simpler, more manageable parts, helping to highlight patterns, reduce noise, or compress data. This makes it easier to perform tasks like identifying trends, making predictions, or compressing information for easier storage and processing.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "Any matrix $A$ can be decomposed into $A = U\\Sigma V^T$ where $U$ and $V$ are orthogonal matrices, and $\\Sigma$ is a diagonal matrix of singular values.\n",
    "\n",
    "**Applications in Machine Learning**: \n",
    "- SVD is used in dimensionality reduction, noise reduction, and data compression.\n",
    "- In machine learning, it's often used to identify latent features in data, as in recommender systems.\n",
    "\n",
    "**Python Example**:\n",
    "```python\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "U, Sigma, VT = np.linalg.svd(A)\n",
    "print(\"U:\\n\", U)\n",
    "print(\"Sigma:\", Sigma)\n",
    "print(\"V^T:\\n\", VT)\n",
    "```\n",
    "\n",
    "#### LU Decomposition\n",
    "\n",
    "##### ELI5 Explanation\n",
    "\n",
    "Imagine you have a big jigsaw puzzle. LU Decomposition is like a strategy for solving this puzzle in an organized way. In this strategy, you divide the puzzle into two types of simpler pieces: \"L\" pieces and \"U\" pieces.\n",
    "\n",
    "1. **\"L\" Pieces (Lower Triangular Matrix)**: These are like the puzzle pieces that only have parts sticking out on their top and right sides, but are flat on the bottom and left sides. When you put these pieces together, they form a shape that looks like a staircase climbing up from left to right. This is the \"L\" part of the decomposition, where \"L\" stands for \"Lower\".\n",
    "\n",
    "2. **\"U\" Pieces (Upper Triangular Matrix)**: These pieces are the opposite. They only have parts sticking out on their bottom and left sides, and are flat on the top and right sides. When you put these together, they form an inverted staircase, going down from left to right. This is the \"U\" part, where \"U\" stands for \"Upper\".\n",
    "\n",
    "Now, why do we do this? In math, especially when solving equations, breaking a problem (like a big matrix) into these \"L\" and \"U\" parts makes it much easier to handle. It's like solving the bottom part of the puzzle first (the \"L\" pieces), and then the top part (the \"U\" pieces). This method can make solving complex mathematical problems more manageable.\n",
    "\n",
    "In practical terms, if you have a set of equations to solve (which can be represented as a matrix), using LU Decomposition allows you to first simplify the problem into an \"L\" part, solve it, and then use that solution to tackle the \"U\" part. It's a step-by-step approach that can be more efficient than trying to solve the whole complex problem at once. \n",
    "\n",
    "In summary, LU Decomposition is a technique to simplify and solve complex problems by breaking them down into easier, structured parts, much like organizing and solving a jigsaw puzzle in a methodical way.\n",
    "\n",
    "**Mathematical Notation**: \n",
    "LU decomposition factors a matrix $A$ into $A = LU$ where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix.\n",
    "\n",
    "**Utility in Solving Linear Equations**: \n",
    "- LU decomposition is used to solve linear equations, invert matrices, and compute determinants.\n",
    "- It is particularly useful for systems of linear equations as it simplifies the process, reducing computational cost.\n",
    "\n",
    "**Python Example**:\n",
    "```python\n",
    "from scipy.linalg import lu\n",
    "\n",
    "A = np.array([[3, 2], [1, 4]])\n",
    "P, L, U = lu(A)\n",
    "print(\"Lower Triangular Matrix L:\\n\", L)\n",
    "print(\"Upper Triangular Matrix U:\\n\", U)\n",
    "```\n",
    "\n",
    "Matrix decomposition techniques like these are fundamental tools in numerical linear algebra and have wide applications in machine learning. They are used for simplifying matrix operations, solving systems of linear equations, and performing dimensionality reduction, among other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformations and Matrices\n",
    "- **Linear Transformations**: Define and explain with examples.\n",
    "- **Representation with Matrices**: How linear transformations can be represented as matrix operations.\n",
    "\n",
    "#### Systems of Linear Equations\n",
    "- **Representation with Matrices**: Using matrices to represent systems of linear equations.\n",
    "- **Solving Linear Systems**: Methods like Gaussian elimination, matrix inversion, and iterative methods.\n",
    "\n",
    "#### Matrix Calculus\n",
    "- **Gradient and Hessian**: Introduce the concepts of gradient and Hessian matrices in the context of optimization.\n",
    "- **Application in Machine Learning**: Discuss how these concepts are used in training models, such as in gradient descent.\n",
    "\n",
    "#### Practical Applications in Machine Learning\n",
    "- **Data Representation**: How data is represented as matrices in various ML algorithms.\n",
    "- **Feature Transformation**: Use of matrices in feature scaling, PCA, and other transformation techniques.\n",
    "- **Neural Networks and Deep Learning**: The role of matrices in the structure and computation of neural networks.\n",
    "\n",
    "#### Exercises and Problems\n",
    "- **Conceptual Questions**: To test understanding of key concepts.\n",
    "- **Applied Problems**: Real-world scenarios where matrix algebra is applied in machine learning.\n",
    "- **Programming Exercises**: Implementing basic matrix operations and algorithms in a programming language commonly used in machine learning, such as Python.\n",
    "\n",
    "#### Further Reading and Resources\n",
    "- **Books and Academic Papers**: A curated list of advanced texts and seminal papers.\n",
    "- **Online Resources**: Tutorials, lectures, and interactive platforms for further learning.\n",
    "\n",
    "### Summary\n",
    "- **Recap of Key Points**: Summarize the most important concepts and their relevance in machine learning.\n",
    "- **Real-World Implications**: Discuss how matrix algebra underpins many modern machine learning technologies and applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
