{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Chatbot with Upstage Solar API Endpoint, Langchain and Gradio\n",
    "\n",
    "This lab goes through the creation of a chatbot with the following details :\n",
    "- Uses the Solar LLM API endpoint from Upstage.\n",
    "- Uses Langchain for simplifying the API calls to Upstage Solar API and to also maintain context (chat history)\n",
    "- Uses Gradio for the simple chatbot UI\n",
    "\n",
    "![chatbot](../../../images/20240301-gradiochatbot.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install the python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q langchain langchain-community langchainhub langchain-openai llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the Solar API Endpoint key and setup the base url\n",
    "\n",
    "Note that you will need to signup for the Solar API endpoint. It is in a free beta until the end of March 2024. From there, you can get your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "BASE_URL = 'https://api.upstage.ai/v1/solar'\n",
    "APIKEY = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Let's test out the Solar API with a simple chat API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello! I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you with any questions or problems you may have. How can I assist you today?\"\n"
     ]
    }
   ],
   "source": [
    "# Import the `ChatOpenAI` class from the `langchain_openai` package. \n",
    "# This class is used to interact with OpenAI's chat models.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Import `HumanMessage` and `SystemMessage` classes from the `langchain.schema` module.\n",
    "# These classes are used to construct messages that simulate a chat interaction between a human and a system.\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "chat_model = ChatOpenAI(base_url=BASE_URL, model_name=\"solar-1-mini-chat\", api_key=APIKEY)\n",
    "\n",
    "# Create a list named `messages` containing instances of `SystemMessage` and `HumanMessage`.\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Hi, how are you?\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Invoke the chat model using the `invoke` method of the `chat_model` object, passing the `messages` list as input.\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "# Print the response received from the chat model to the console.\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. And here is the code for the full chatbot\n",
    "\n",
    "- I think this code is pretty self explanatory.\n",
    "- Note that it won't work necessarily in a cloud hosted notebook (like on Azure or maybe Google Colab). I don't think so anyway. The gradio chatbot assumes localhost. Personally, I did this with vscode and it worked quite well. Gardio runs very well on vscode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# chat_model sets up the atual chatbot API from upstage Solar\n",
    "chat_model = ChatOpenAI(base_url=BASE_URL, model_name=\"solar-1-mini-chat\", api_key=APIKEY)\n",
    "\n",
    "# chat_history is a Langchain feature that helps manage chat history context\n",
    "chat_history = ChatMessageHistory()\n",
    "\n",
    "# we are using another langchain feature that creates a reusable prompt\n",
    "# The first part of this defines the context for the chatbot and then \n",
    "# MessagesPlaceholder allows us to inject the chat history with the \n",
    "# users most recent prompt appended on. \n",
    "# This method continues to take messages and responses and accumulate\n",
    "# context over the entire conversation.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# This is a big part of langchain, chaining together the prompt and the model (API)\n",
    "chain = prompt | chat_model\n",
    "\n",
    "# This doResponse function takes a user prompt and then\n",
    "# injects the prompt (with some additional context) onto the chat history\n",
    "# then sends the entire message history + prompt to the Solar API\n",
    "# This function gets called by the Gradio Chatbot interface (see below)\n",
    "def doResponse(user_prompt):\n",
    "    global chat_model, chat_history\n",
    "\n",
    "    chat_history.add_user_message((\"(act as human friend to user. \"\n",
    "                                   \"Don't act like an ai. \"\n",
    "                                   \"Don't act like an assistant. \" \n",
    "                                   \"Do not break character) user prompt: \") + user_prompt)\n",
    "\n",
    "    response = chain.invoke({\"messages\": chat_history.messages})\n",
    "\n",
    "    chat_history.add_ai_message(response)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# The rest of this code uses Gradio to create a chatbot UI\n",
    "# The Gradio interface calls the doResponse function with the \n",
    "# user's prompt and updates the chat window with the history\n",
    "# that it is tracking itself.\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(height=300)\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "    def respond(message, ch):\n",
    "        time.sleep(2)\n",
    "        bot_message = doResponse(message)\n",
    "        ch.append((message, bot_message))\n",
    "        return \"\", ch\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
